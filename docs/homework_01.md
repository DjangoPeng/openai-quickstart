## 单项选择题

1. 注意力机制（Attention）的主要用途是什么？
   - A. 优化模型训练速度
   - B. 提高模型准确率
   - C. 选择重要的信息并忽略不相关的信息
   - D. 改进模型的可解释性

2. Transformer模型是基于什么理论构建的？
   - A. 递归神经网络（RNN）
   - B. 卷积神经网络（CNN）
   - C. 注意力机制（Attention）
   - D. 自组织映射（SOM）

3. GPT和BERT的主要区别是什么？
   - A. GPT是基于Transformer的，而BERT不是
   - B. BERT是基于Transformer的，而GPT不是
   - C. GPT使用了单向自注意力，而BERT使用了双向自注意力
   - D. GPT和BERT在基本结构上没有区别

4. 在注意力机制中，“Q”、“K”和“V”分别代表什么？
   - A. 查询、密钥和值
   - B. 查询、键入和验证
   - C. 快速、关键和验证
   - D. 问题、知识和视觉

5. Transformer模型是如何解决长距离依赖问题的？
   - A. 通过递归神经网络（RNN）
   - B. 通过卷积神经网络（CNN）
   - C. 通过注意力机制（Attention）
   - D. 通过自组织映射（SOM）

6. GPT主要用于哪种类型的任务？
   - A. 分类任务
   - B. 回归任务
   - C. 生成任务
   - D. 聚类任务

7. 以下哪项是BERT的主要创新之处？
   - A. 引入了自注意力机制
   - B. 使用了双向自注意力机制
   - C. 提出了新的优化算法
   - D. 突破了模型大小的限制

8. 在Transformer模型中，自注意力机制的主要作用是什么？
   - A. 加速模型训练
   - B. 识别输入中的关键信息
   - C. 生成高质量的词嵌入
   - D. 提高模型的鲁棒性

9. 基于Transformer的模型，如GPT和BERT，主要适用于哪些任务？
   - A. 图像识别
   - B. 自然语言处理
   - C. 语音识别
   - D. 强化学习

10. 注意力机制最早是在哪个领域得到应用的？
    - A. 计算机视觉
    - B. 语音识别
    - C. 自然语言处理
    - D. 推荐系统

## 多项选择题

11. 以下哪些方法被用于处理序列数据？
    - A. 递归神经网络（RNN）
    - B. 卷积神经网络（CNN）
    - C. 注意力机制（Attention）
    - D. 支持向量机（SVM）

12. 以下哪些模型使用了注意力机制？
    - A. BERT
    - B. GPT
    - C. LeNet
    - D. ResNet

13. 以下哪些模型主要用于自然语言处理任务？
    - A. GPT
    - B. BERT
    - C. VGG
    - D. LeNet

14. 下列哪些说法正确描述了注意力机制的作用？
    - A. 它可以用来改进模型的训练速度
    - B. 它可以用来挑选出重要的信息并忽略不相关的信息
    - C. 它可以用来生成高质量的词嵌入
    - D. 它可以用来提高模型的鲁棒性

15. 下列哪些说法正确描述了BERT模型？
    - A. BERT模型是基于Transformer的
    - B. BERT模型使用了双向自注意力机制
    - C. BERT模型主要用于图像分类任务
    - D. BERT模型突破了模型大小的限制


附加题：

使用 ChatGPT 帮助解答本次课程中未完全理解的概念，并将问答对单独整理成文件或线上文档。示例如下：

```
问：Bert是基于编码器，GPT是基于解码器，不是编码和解码一块用吗？

答：BERT（Bidirectional Encoder Representations from Transformers）和GPT（Generative Pretrained Transformer）确实分别采用了基于编码器和解码器的结构，但他们的运用方式有所不同。

BERT利用了Transformer中的编码器结构，编码器能够处理输入序列的所有元素，然后输出一个新的已经被处理过的序列。最重要的是，编码器在处理序列的时候是双向的，也就是说，在处理某个元素时，它会同时考虑到该元素的上下文，包括它前面和后面的元素。这就使得BERT在处理自然语言处理任务时，具有更好的上下文理解能力。

而GPT则采用了Transformer中的解码器结构，这个结构通常被用于生成序列。与编码器不同的是，解码器在处理序列的时候是单向的，即在处理某个元素时，只会考虑到该元素之前的元素，不会考虑到后面的元素。这就使得GPT非常适合用于生成型的自然语言处理任务，如文本生成、对话系统等。

至于您提到的编码和解码一块使用，通常出现在seq2seq（序列到序列）模型中，例如机器翻译或者文本摘要等任务，输入序列首先通过编码器处理，然后解码器生成输出序列。这种模式并没有在BERT或GPT中直接使用，而是被一些其它的模型，如T5或BART所使用。
```
